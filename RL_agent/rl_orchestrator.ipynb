{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bad3f21-1991-43c7-b840-a61b8073ca89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import pickle\n",
    "\n",
    "import base64\n",
    "import imageio\n",
    "import IPython\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import PIL.Image\n",
    "import pyvirtualdisplay\n",
    "import reverb\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tf_agents.agents.reinforce import reinforce_agent\n",
    "from tf_agents.drivers import py_driver\n",
    "from tf_agents.environments import suite_gym\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.networks import actor_distribution_network\n",
    "from tf_agents.policies import py_tf_eager_policy\n",
    "from tf_agents.replay_buffers import reverb_replay_buffer\n",
    "from tf_agents.replay_buffers import reverb_utils\n",
    "from tf_agents.specs import tensor_spec\n",
    "from tf_agents.trajectories import trajectory\n",
    "from tf_agents.utils import common\n",
    "\n",
    "import random\n",
    "import os\n",
    "import subprocess\n",
    "from itertools import permutations \n",
    "from itertools import combinations \n",
    "import sys\n",
    "from random import randint\n",
    "from datetime import datetime\n",
    "\n",
    "from tf_agents.policies import policy_saver\n",
    "from tf_agents.policies import py_tf_eager_policy\n",
    "from tf_agents.policies import random_tf_policy\n",
    "\n",
    "\n",
    "\n",
    "import abc\n",
    "from tf_agents.environments import py_environment\n",
    "from tf_agents.environments import tf_environment\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.environments import utils\n",
    "from tf_agents.specs import array_spec\n",
    "from tf_agents.environments import wrappers\n",
    "from tf_agents.environments import suite_gym\n",
    "from tf_agents.trajectories import time_step as ts\n",
    "\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379d429f-acaa-4b54-8a92-7640e506855a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f679cf-e7bc-450c-b7b4-118fbb9b971f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restart monitor\n",
    "kill_pcm()\n",
    "start_pcm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c3f4e2-e893-4308-9738-8c3a4b272947",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8701e4c8-71fd-4bf1-bb18-98aa1e13bda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Values\n",
    "\n",
    "stored = 0\n",
    "num_execution_core = 6\n",
    "NUM_CPUS = 24\n",
    "performance_counter_keys = ['MEM_INST_RETIRED.ALL_LOADS', 'L1D.REPLACEMENT', 'MEM_LOAD_RETIRED.L2_MISS', 'MEM_LOAD_RETIRED.L3_MISS', 'IPS', 'L1_MISS', 'L2_MISS', 'L3_MISS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383c2ce8-e098-480a-98b5-4d083c8822e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157924ca-c54f-42f0-926e-fb7c6b41af21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get HPC and pre-process HPC\n",
    "\n",
    "def profiling_cpu(df_dict):\n",
    "    cpu_counters = []\n",
    "    for df in performance_counter_keys:\n",
    "        #rint(df_dict[df].iloc[0])\n",
    "        for df_i in df_dict[df].iloc[0]:\n",
    "            cpu_counters.append(df_i)\n",
    "\n",
    "    #print(cpu_counters)\n",
    "    cpu_counters_counter = []\n",
    "\n",
    "    for counter in range(int(len(cpu_counters)/NUM_CPUS)):\n",
    "        this_cpu_counter = []\n",
    "        for cpu in range(NUM_CPUS):\n",
    "            this_cpu_counter.append(cpu_counters[cpu+NUM_CPUS*counter])\n",
    "        cpu_counters_counter.append(this_cpu_counter)\n",
    "    \n",
    "    # NORMALIZE\n",
    "    max_list = []\n",
    "    for i in range(len(cpu_counters_counter)):\n",
    "        max_c = 0\n",
    "        for j in range(NUM_CPUS):\n",
    "            if cpu_counters_counter[i][j] > max_c:\n",
    "                max_c = cpu_counters_counter[i][j]\n",
    "        max_list.append(max_c)\n",
    "\n",
    "    \n",
    "    cpu_counters_normalized = []\n",
    "    for i in range(len(cpu_counters_counter)):\n",
    "        cpu_counters_normalized_this = []\n",
    "        for j in range(NUM_CPUS):\n",
    "            cpu_counters_normalized_this.append(cpu_counters_counter[i][j]/max_list[i]/2)\n",
    "        cpu_counters_normalized.append(cpu_counters_normalized_this)\n",
    "    return cpu_counters_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7cd1841-0eb5-4dfc-b8cf-8f12c811a0c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08fd439-765a-4970-9d38-e169a628f492",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update HPC\n",
    "\n",
    "def update_observation():\n",
    "    global stored\n",
    "    while(True):\n",
    "        try:\n",
    "            df_dict = get_microarchitecture_snapshot_2()\n",
    "            if df_dict['L3_MISS'].iloc[-1][0] != stored:\n",
    "                #print(df_dict['L3_MISS'].iloc[-1][0])\n",
    "                stored = df_dict['L3_MISS'].iloc[-1][0]\n",
    "                #profiling_input(df_dict.iloc[-1])\n",
    "                break\n",
    "            else:\n",
    "                print(\"no update collected\")\n",
    "                time.sleep(1)\n",
    "        except Exception as e:\n",
    "            print(\"no data collected\", e)\n",
    "            time.sleep(1)\n",
    "    global num_execution_core\n",
    "    observation = profiling_cpu(df_dict)\n",
    "    observation.append([num_execution_core/NUM_CPUS]*24)\n",
    "    return observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3a01d1-9d25-480c-9dea-b513da512668",
   "metadata": {},
   "outputs": [],
   "source": [
    "hardware_counters = update_observation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad91e98-7c2f-489c-a43f-91a4def4ca5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update deployment\n",
    "\n",
    "def get_current_execution_state():\n",
    "    return num_execution_core\n",
    "\n",
    "def update_current_execution_state(op):\n",
    "    global num_execution_core\n",
    "    if op == 1:\n",
    "        if num_execution_core >= 20:\n",
    "            return -1\n",
    "        else:\n",
    "            num_execution_core += 1\n",
    "    else:\n",
    "        if num_execution_core <= 4:\n",
    "            return -1\n",
    "        else:\n",
    "            num_execution_core -=1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc098d4-c267-4347-8b31-5dd8f9ea6d2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82eab515-4b2b-49ea-a303-9bf65362c59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_deployment():\n",
    "    nflambda = NFlambda(start = True)\n",
    "    \n",
    "    nflambda.parameter = Parameter()\n",
    "    nflambda.parameter.d[\"state_size\"] = 1000000\n",
    "    nflambda.parameter.d[\"data_state\"] = 4000\n",
    "    nflambda.parameter.d[\"control_state\"] = 100\n",
    "    nflambda.parameter.d[\"control_frequency\"] = 5\n",
    "    nflambda.parameter.d[\"control_complexity\"] = 500 # Tested: 500\n",
    "    nflambda.configure_parameter(nflambda.parameter)\n",
    "    for i in nfunctor_graph.state_runtime_table.data:\n",
    "        state_id = nfunctor_graph.state_runtime_table.data.index(i)\n",
    "        nflambda.send_command(\n",
    "                command_state.with_argument([\"create\", str(0), str(state_id)]))\n",
    "        \n",
    "        \n",
    "    decomposed = False    \n",
    "    # decomposed deployment\n",
    "    if(decomposed):\n",
    "        for i in nfunctor_graph.action_runtime_table.data:\n",
    "            if i.is_data():\n",
    "                i.core_id = i.execution_id*2 + 1\n",
    "            else:\n",
    "                i.core_id = 48 - i.execution_id*2 - 1\n",
    "        \n",
    "    else:\n",
    "        global num_execution_core\n",
    "        num_control_core = num_execution_core\n",
    "        num_data_core = 24 - num_execution_core\n",
    "\n",
    "        print(\"deploying \")\n",
    "        avaliable_cores_data = []\n",
    "        avaliable_cores_control = []\n",
    "        for i in range(num_data_core):\n",
    "            avaliable_cores_data.append((i*2+1)%48)\n",
    "        for i in range(num_execution_core):\n",
    "            avaliable_cores_control.append((i*2 + num_data_core*2 + 1)%48)\n",
    "\n",
    "        data_core_index = 0\n",
    "        control_core_index = 0\n",
    "        for i in nfunctor_graph.action_runtime_table.data:\n",
    "            if i.is_data():\n",
    "                i.core_id = avaliable_cores_data[i.execution_id % num_data_core]\n",
    "            else:\n",
    "                i.core_id = avaliable_cores_control[i.execution_id % num_control_core]\n",
    "            #print(i.core_id, i.execution_id)\n",
    "\n",
    "    \n",
    "    # install action\n",
    "    for i in nfunctor_graph.action_runtime_table.data:\n",
    "        action_index = nfunctor_graph.action_runtime_table.data.index(i)\n",
    "        if(i.action.action.name ==\"packet_in\"):\n",
    "            pass\n",
    "\n",
    "        elif(i.action.action.name ==\"packet_out\"):\n",
    "            pass\n",
    "        else:\n",
    "            #print(i.action.action.registration_id,action_index,i.state_id, i.core_id)\n",
    "            nflambda.send_command(command_action.with_argument(\n",
    "                [\"create\", str(i.action.action.registration_id), str(action_index),\n",
    "                 str(i.state_id), str(i.core_id)]))\n",
    "            pass\n",
    "    \n",
    "    # install action\n",
    "    for i in nfunctor_graph.action_runtime_table.data:\n",
    "        action_index = nfunctor_graph.action_runtime_table.data.index(i)\n",
    "        if(i.action.action.name ==\"packet_in\"):\n",
    "            nflambda.send_command(\n",
    "                command_config.with_argument([\"rss_reta_config\", str(i.starting_bucket), str(i.num_bucket), str(i.core_id), str(action_index)]))\n",
    "\n",
    "            next_action = i.next_action\n",
    "            nflambda.send_command(command_worker.with_argument(\n",
    "                [\"first_action\", str(i.core_id), str(next_action), str(i.starting_bucket), str(i.num_bucket)]))\n",
    "\n",
    "        elif(i.action.action.name ==\"packet_out\"):\n",
    "            #print([\"create_packet_out\", str(action_index), str(i.core_id)])\n",
    "            # configure last action\n",
    "            nflambda.send_command(command_action.with_argument([\"create_packet_out\", str(action_index), str(i.core_id)]))\n",
    "            pass\n",
    "        else:\n",
    "            #print(i.action.action.registration_id,action_index,i.state_id, i.core_id)\n",
    "            nflambda.send_command(command_action.with_argument(\n",
    "                [\"create\", str(i.action.action.registration_id), str(action_index),\n",
    "                 str(i.state_id), str(i.core_id)]))\n",
    "            pass\n",
    "    \n",
    "    # print(action_id,action_index,state_index,)\n",
    "    \n",
    "    # install path\n",
    "    for i in nfunctor_graph.state_runtime_table.data:\n",
    "        state_id = nfunctor_graph.state_runtime_table.data.index(i)\n",
    "\n",
    "        nflambda.send_command(\n",
    "                command_state.with_argument([\"create\", str(0), str(state_id)]))\n",
    "\n",
    "        for j in i.path:\n",
    "            if j!=-1:\n",
    "                path_index = i.path.index(j)\n",
    "                path_id = j\n",
    "                # print(path_index,path_id,state_id)        \n",
    "                nflambda.send_command(command_state.with_argument([\"install_simple_path\", str(state_id), str(j), str(path_index)]))\n",
    "                \n",
    "                \n",
    "\n",
    "    sleep(1)    \n",
    "    nflambda.run()\n",
    "    \n",
    "\n",
    "    \n",
    "    sleep(5)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da89348a-6d67-44dc-a04b-21e219092ea1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53216af6-c54b-4d82-99bc-109d56c928a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get throughput\n",
    "# return value * 1.8 = actual throuput (Gb/s)\n",
    "def get_throughput():\n",
    "    throughput_sum = 0\n",
    "    while ((throughput_sum <=8) or (throughput_sum >=75)):\n",
    "        throughput = []\n",
    "        get_flag = 0\n",
    "        while(get_flag == 0):\n",
    "            try:\n",
    "                temp = get_nflambda_log()\n",
    "                get_flag = 1\n",
    "            except:\n",
    "                print(\"trying restart...\")\n",
    "                restart_nf()\n",
    "                update_deployment()\n",
    "                sleep(10)\n",
    "\n",
    "        #throughput.append(temp['worker']['1_throughput'][-1])\n",
    "        index = 1\n",
    "        throughput_sum = 0\n",
    "        while index <= 47:\n",
    "            throughput.append(temp['worker'][str(index)+'_throughput'][-1])\n",
    "            throughput_sum += temp['worker'][str(index)+'_throughput'][-1]\n",
    "            index += 2\n",
    "    return throughput_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a95d4a7-c3ef-4b55-a2f3-43c6e768f8a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf35e5a5-4360-4a84-8261-02b8885d05d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize RL state\n",
    "\n",
    "def state_initialize():\n",
    "    global num_execution_core\n",
    "    num_execution_core = 6\n",
    "    update_deployment()\n",
    "    return update_observation()\n",
    "\n",
    "def state_initialize_training():\n",
    "    global num_execution_core\n",
    "    num_execution_core = 6\n",
    "    return get_counter_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b1ac34-c89c-4f5a-924a-417603c053e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b992ccaa-345f-4e99-9675-0f57c47db550",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sweep test\n",
    "num_execution_core = 4\n",
    "performance_sweep = {}\n",
    "observation_sweep = {}\n",
    "for core_n in range(17):\n",
    "    print(\"current control core:\", num_execution_core)\n",
    "    throughput_avg = 0\n",
    "    sleep(1)\n",
    "    update_deployment()\n",
    "    sleep(2)\n",
    "    throughput_avg = get_throughput()\n",
    "    sleep(1)\n",
    "    state_new = update_observation()\n",
    "    observation_sweep[num_execution_core] = state_new\n",
    "    performance_sweep[num_execution_core] = throughput_avg\n",
    "    num_execution_core += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35581928-5edc-479f-a1f9-93fd1ca71c5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e8b918-8c13-4556-b150-7128459b5609",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For faster training, record the results, and generate random noise\n",
    "\n",
    "import random\n",
    "def get_performance_training():\n",
    "    core = num_execution_core\n",
    "    performance_obs = performance_sweep[core] * random.uniform(0.98,1.02)\n",
    "    return performance_obs\n",
    "    \n",
    "def get_counter_training():\n",
    "    core = num_execution_core\n",
    "    counter = observation_sweep[core]\n",
    "    counter_obs = []\n",
    "    for row in counter[:-1]:\n",
    "        row_obs = []\n",
    "        for value in row:\n",
    "            row_obs.append(value * random.uniform(0.95,1.05))\n",
    "        counter_obs.append(row_obs)\n",
    "    counter_obs.append(counter[-1])\n",
    "    return counter_obs\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5599a331-2100-4479-b524-d7ff964d4300",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc2d587-f2a7-4ccb-8c76-330b735edda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9a5b06-2db2-4ac6-adb2-afc7ee5f589e",
   "metadata": {},
   "outputs": [],
   "source": [
    "OBSERVATION_SPACESHAPE = len(hardware_counters)*len(hardware_counters[0]) # only contains cpu_counters\n",
    "MAXIMUM_UPDATE_NUM = 12 # larger value results as easier transfer onto a new environment (and better performance once trained), smaller value make faster convergence  \n",
    "IMMEDIATE_UPDATE_THRESHOLD = 1.1 # immediately update the agent when an action generates obviously better performance\n",
    "IMMEDIATE_ROLLBACK_THRESHOLD = 0.9 # immediately rollback an action if it results as a significantly worse performance\n",
    "GAIN_AMP = 32 # amplifier to give non-linear rewards for actions\n",
    "GAIN_EMP = 128 # amplifier to give more significant rewards for outstanding good actions.\n",
    "UPDATE_LIMIT = 1.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce79ef51-2258-4dc6-9161-66339b42dd91",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iterations = 128\n",
    "collect_episodes_per_iteration = MAXIMUM_UPDATE_NUM \n",
    "replay_buffer_capacity = 8192 \n",
    "fc_layer_params = (128,)\n",
    "learning_rate = 1e-3 \n",
    "log_interval = 2 \n",
    "num_eval_episodes = 2\n",
    "eval_interval = 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e86a3e7-5ec8-4d25-bd33-bf30ae3a63ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbd3507-6768-4766-8fa2-397a70a1ca0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "logaction = open(\"logged_actions_10.txt\",'a')\n",
    "\n",
    "class NFUpdateEnv_BIN(py_environment.PyEnvironment):\n",
    "\n",
    "  def __init__(self):\n",
    "\n",
    "    # Action: possible updates\n",
    "    # Simpified as a binary operation, \n",
    "    # 0 relocates a data core to control, 1 relocates a control core to data\n",
    "    self._action_spec = array_spec.BoundedArraySpec(\n",
    "        shape=(1,), dtype=np.int32, minimum=0, maximum=1, name='action')\n",
    "    \n",
    "    # Observation: normalized hardware performance counters\n",
    "    self._observation_spec = array_spec.BoundedArraySpec(\n",
    "        shape=(len(hardware_counters),len(hardware_counters[0])), dtype=np.float32, minimum=0, maximum=1, name='observation')\n",
    "    \n",
    "    # Train with random noise\n",
    "    self._state = state_initialize_training()\n",
    "    # Train with real task\n",
    "    self._state = state_initialize()\n",
    "\n",
    "    \n",
    "    self._search_ended = False\n",
    "    self._num_ite = 0\n",
    "    self._search_ended_abn = 0\n",
    "    self.best_result = 1\n",
    "    self.ite_optimal = 0\n",
    "    self.ite_best = 0\n",
    "    self._step_counter = 0\n",
    "    self._gain_record = 0\n",
    "    self._hm_record = []\n",
    "    self._good_search = 0\n",
    "    self._new_record = 0    \n",
    "    \n",
    "    \n",
    "  def step_counter(self):\n",
    "    return self._step_counter\n",
    "\n",
    "  def action_spec(self):\n",
    "    return self._action_spec\n",
    "\n",
    "  def observation_spec(self):\n",
    "    return self._observation_spec\n",
    "\n",
    "  def _reset(self):\n",
    "    print(\"RESET...\")\n",
    "    # Train with random noise\n",
    "    self._state = state_initialize_training()\n",
    "    # Train with real task\n",
    "    self._state = state_initialize()\n",
    "\n",
    "    self._search_ended = False\n",
    "    self._num_ite = 0\n",
    "    self.ite_best += 1\n",
    "    self.ite_optimal += 1\n",
    "    self._search_ended_abn = 0\n",
    "    self._gain_record = []\n",
    "    self._good_search = 0\n",
    "    self._new_record = 0\n",
    "    return ts.restart(np.array(self._state[:OBSERVATION_SPACESHAPE], dtype=np.float32))\n",
    "    logaction.write(\"RESET\\n\")\n",
    "    logaction.flush()\n",
    "    \n",
    "  def _reset_op(self):\n",
    "    # Train with random noise\n",
    "    self._state = state_initialize_training()\n",
    "    # Train with real task\n",
    "    self._state = state_initialize()\n",
    "\n",
    "    self._search_ended = False\n",
    "    #self._num_ite = 0\n",
    "    return ts.termination(np.array(self._state[:OBSERVATION_SPACESHAPE], dtype=np.float32), reward=(4))\n",
    "\n",
    "\n",
    "  def _step(self, action):\n",
    "    print(\"ACTION GENERATED:\",action)\n",
    "    #print(self._num_ite)\n",
    "    gain_amp = 0\n",
    "    self._step_counter += 1\n",
    "\n",
    "    if self._search_ended:\n",
    "      print(\"SEARCH ENDED\",self._state)\n",
    "      #wait = input()\n",
    "      logsearch = open(\"logged_search_10.txt\",'a')\n",
    "      logsearch.write(\"STEP COUNTER: \" + str(self._step_counter) + \"  >  GAIN RECORD: \" + str(self._gain_record) + \"\\n\")\n",
    "      logsearch.flush()\n",
    "      return self.reset()\n",
    "\n",
    "    \n",
    "\n",
    "    if self._num_ite >= MAXIMUM_UPDATE_NUM:\n",
    "        if self._new_record <= self._throughput_initial :\n",
    "              print(\"SEARCH ENDED WITH MAXIMUM TRY, NEGATIVE REWARD: \", -1)\n",
    "              self._search_ended_abn = 2\n",
    "              self._search_ended = True\n",
    "        else:\n",
    "              print(\"SEARCH ENDED WITH MAXIMUM TRY, POSITIVE REWARD: \", 1)\n",
    "              self._search_ended_abn = 4\n",
    "              self._search_ended = True            \n",
    "      #return ts.termination(np.array(self._state, dtype=np.float32), reward=(-8))\n",
    "\n",
    "\n",
    "    if self._num_ite == 0:\n",
    "        # Train with random noise\n",
    "        self._throughput_initial = get_performance_training()\n",
    "        # Train with real task\n",
    "        self._throughput_initial = get_throughput()\n",
    "        \n",
    "        self._throughput_old = self._throughput_initial\n",
    "        \n",
    "    update_flag = update_current_execution_state(action)\n",
    "    \n",
    "    if update_flag != -1:\n",
    "        #update_deployment()\n",
    "        # Train with random noise\n",
    "        state_new = get_counter_training()\n",
    "        # Train with real task\n",
    "        state_new = update_observation()\n",
    "\n",
    "        # Train with random noise\n",
    "        throughput_new = get_performance_training()\n",
    "        # Train with real task        \n",
    "        # Wait until Stable\n",
    "        sleep(1)\n",
    "        throughput_new = get_throughput()\n",
    "        \n",
    "        while ((throughput_new >= 75) or (throughput_new <= 10)):\n",
    "                    # Train with random noise\n",
    "                    throughput_new = get_performance_training()\n",
    "                    # Train with real task        \n",
    "                    # Wait until Stable\n",
    "                    sleep(1)\n",
    "                    throughput_new = get_throughput()\n",
    "                    print(\"bad value, re-test...\")\n",
    "        if throughput_new >= self._throughput_initial * 1.02:\n",
    "            self._good_search = 1\n",
    "        self._state = state_new\n",
    "        gain = throughput_new / self._throughput_old\n",
    "        print(\"ACTION TAKE:\", action)\n",
    "        print(\"PERFORMANCE UPDATE:\", self._throughput_old, \"-->\", throughput_new)\n",
    "        #print(\"STATE:\", self._state)\n",
    "        self._throughput_old = throughput_new\n",
    "        logaction.write(\"ACTION:\"+str(action)+\" - NUM_CONTROL:\"+str(num_execution_core) + \" - THROUGHPUT:\" + str(throughput_new) + \"\\n\")\n",
    "        logaction.flush()\n",
    "        self._new_record = throughput_new\n",
    "\n",
    "    else:\n",
    "        gain = 1.0\n",
    "        throughput_new = self._throughput_old\n",
    "        print(\"ACTION NOT TAKE (INVALID):\", action)\n",
    "        logaction.write(\"ACTION:\"+str(action)+\" - NUM_CONTROL:\"+str(num_execution_core) + \" - INVALID\" + \"\\n\")\n",
    "        logaction.flush()\n",
    "    #gain_o = gain_o[0]\n",
    "    #gain_n = gain_n[0]\n",
    "\n",
    "    self._gain_record.append(throughput_new)\n",
    "    if gain >= 1.02:\n",
    "      #self._state = state_update(self._state, action)\n",
    "      if throughput_new > self.best_result * UPDATE_LIMIT: \n",
    "        self.best_result = throughput_new\n",
    "        print(\"NEW BEST\", self.best_result, \"IN ITE\", self._step_counter)\n",
    "        logbest = open(\"logged_best_10.txt\",'a')\n",
    "        logbest.write(str(self.best_result) + \" > \" + str(self._state) + \" > \" + str(self._step_counter) + \"\\n\")\n",
    "        logbest.flush()\n",
    "        print(\"SEARCH ENDED WITH OPTIMAL PLAN\")\n",
    "        #wait = input()\n",
    "        print(\"NEW OPT\", throughput_new, \"IN ITE\", self.ite_optimal)\n",
    "        self.ite_best = 0\n",
    "        self.ite_optimal = 0\n",
    "        self._search_ended = True\n",
    "    #else:\n",
    "    #  self._state = state_update(self._state, action)\n",
    "    \n",
    "    \n",
    "    self._num_ite += 1\n",
    "    self._observation_spec = self._state[:OBSERVATION_SPACESHAPE]\n",
    "\n",
    "    # Reinforcement Gain\n",
    "    RGAIN = 16\n",
    "\n",
    "    if self._search_ended:\n",
    "        if self._search_ended_abn == 1:\n",
    "              return ts.termination(np.array(self._state[:OBSERVATION_SPACESHAPE], dtype=np.float32), reward=(-1))\n",
    "        if self._search_ended_abn == 2:\n",
    "              return ts.termination(np.array(self._state[:OBSERVATION_SPACESHAPE], dtype=np.float32), reward=(-1)) \n",
    "        if self._search_ended_abn == 4:\n",
    "              return ts.termination(np.array(self._state[:OBSERVATION_SPACESHAPE], dtype=np.float32), reward=(1)) \n",
    "        if self._search_ended_abn == 0:\n",
    "              gain = throughput_new / self._throughput_initial\n",
    "              reward = (((gain)**2)) * RGAIN\n",
    "              print(\"ENHANCED POSITIVE REWARD:\", reward)\n",
    "              return ts.termination(np.array(self._state[:OBSERVATION_SPACESHAPE], dtype=np.float32), reward)\n",
    "    else:\n",
    "      if gain > 1:\n",
    "          print(\"POSITIVE REWARD:\",(((gain)**2)-1)*10)\n",
    "          return ts.transition(\n",
    "              np.array(self._state[:OBSERVATION_SPACESHAPE], dtype=np.float32), reward=(((gain)**2-1)*10), discount=0.9)\n",
    "      elif gain == 1.0:\n",
    "          print(\"INVALID ACTION, -1 REWARD\")\n",
    "          return ts.transition(\n",
    "              np.array(self._state[:OBSERVATION_SPACESHAPE], dtype=np.float32), reward=-1, discount=1.0)          \n",
    "      else:\n",
    "          print(\"NEGATIVE REWARD:\",(((1/gain)**2)-1)*10)\n",
    "          #return ts.transition(\n",
    "          #    np.array(self._state[:OBSERVATION_SPACESHAPE], dtype=np.float32), reward=-(64**(1/gain)/64)+1, discount=1.0)\n",
    "          return ts.transition(\n",
    "              np.array(self._state[:OBSERVATION_SPACESHAPE], dtype=np.float32), reward=-(((1/gain)**2-1)*10), discount=0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd53050-3b3e-46ba-a60f-b950efb8a0a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a026e40e-6957-4b5e-9e63-09b77bc8d2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = NFUpdateEnv_BIN()\n",
    "global_step = tf.compat.v1.train.get_or_create_global_step()\n",
    "action = np.array([1], dtype=np.int32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcea45bd-c65f-44d4-9c20-eaa3569fb03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_py_env = env\n",
    "eval_py_env = env\n",
    "\n",
    "train_env = tf_py_environment.TFPyEnvironment(train_py_env)\n",
    "eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)\n",
    "\n",
    "actor_net = actor_distribution_network.ActorDistributionNetwork(\n",
    "    train_env.observation_spec(),\n",
    "    train_env.action_spec(),\n",
    "    fc_layer_params=fc_layer_params)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "train_step_counter = tf.Variable(0)\n",
    "\n",
    "tf_agent = reinforce_agent.ReinforceAgent(\n",
    "    train_env.time_step_spec(),\n",
    "    train_env.action_spec(),\n",
    "    actor_network=actor_net,\n",
    "    optimizer=optimizer,\n",
    "    normalize_returns=True,\n",
    "    train_step_counter=train_step_counter)\n",
    "#tf_agent.initialize()\n",
    "\n",
    "policy = tf_agent.policy\n",
    "eval_policy = tf_agent.policy\n",
    "collect_policy = tf_agent.collect_policy\n",
    "\n",
    "policy_checkpointer = common.Checkpointer(ckpt_dir=os.path.join(\"./ckpts/\", 'checkpoint_test'),\n",
    "                                                policy=policy)\n",
    "policy_checkpointer.initialize_or_restore()\n",
    "\n",
    "eval_policy = tf_agent.policy\n",
    "collect_policy = tf_agent.collect_policy\n",
    "\n",
    "def compute_avg_return(environment, policy, num_episodes=10):\n",
    "\n",
    "  total_return = 0.0\n",
    "  for _ in range(num_episodes):\n",
    "\n",
    "    time_step = environment.reset()\n",
    "    episode_return = 0.0\n",
    "\n",
    "    while not time_step.is_last():\n",
    "      action_step = policy.action(time_step)\n",
    "      time_step = environment.step(action_step.action)\n",
    "      episode_return += time_step.reward\n",
    "    total_return += episode_return\n",
    "\n",
    "  avg_return = total_return / num_episodes\n",
    "  return avg_return.numpy()[0]\n",
    "\n",
    "\n",
    "table_name = 'uniform_table'\n",
    "replay_buffer_signature = tensor_spec.from_spec(\n",
    "      tf_agent.collect_data_spec)\n",
    "replay_buffer_signature = tensor_spec.add_outer_dim(\n",
    "      replay_buffer_signature)\n",
    "table = reverb.Table(\n",
    "    table_name,\n",
    "    max_size=replay_buffer_capacity,\n",
    "    sampler=reverb.selectors.Uniform(),\n",
    "    remover=reverb.selectors.Fifo(),\n",
    "    rate_limiter=reverb.rate_limiters.MinSize(1),\n",
    "    signature=replay_buffer_signature)\n",
    "\n",
    "reverb_server = reverb.Server([table])\n",
    "\n",
    "replay_buffer = reverb_replay_buffer.ReverbReplayBuffer(\n",
    "    tf_agent.collect_data_spec,\n",
    "    table_name=table_name,\n",
    "    sequence_length=None,\n",
    "    local_server=reverb_server)\n",
    "\n",
    "rb_observer = reverb_utils.ReverbAddEpisodeObserver(\n",
    "    replay_buffer.py_client,\n",
    "    table_name,\n",
    "    replay_buffer_capacity\n",
    ")\n",
    "\n",
    "def collect_episode(environment, policy, num_episodes):\n",
    "\n",
    "  driver = py_driver.PyDriver(\n",
    "    environment,\n",
    "    py_tf_eager_policy.PyTFEagerPolicy(\n",
    "      policy, use_tf_function=True),\n",
    "    [rb_observer],\n",
    "    max_episodes=num_episodes)\n",
    "  initial_time_step = environment.reset()\n",
    "  driver.run(initial_time_step)\n",
    "\n",
    "print(\"Loaded weights\")\n",
    "print(actor_net.trainable_weights[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e9fd50-df5e-417b-9bdc-baffc30082a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b39bf0-5303-4a89-800b-c5d174bac22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_agent.train = common.function(tf_agent.train)\n",
    "\n",
    "# Reset the train step\n",
    "tf_agent.train_step_counter.assign(0)\n",
    "\n",
    "avg_return = compute_avg_return(eval_env, tf_agent.policy, num_eval_episodes)\n",
    "returns = [avg_return]\n",
    "\n",
    "for _ in range(num_iterations):\n",
    "\n",
    "  collect_episode(\n",
    "      train_py_env, tf_agent.collect_policy, collect_episodes_per_iteration)\n",
    "\n",
    "iterator = iter(replay_buffer.as_dataset(sample_batch_size=1))\n",
    "  trajectories, _ = next(iterator)\n",
    "  #print(iterator)\n",
    "  #print(trajectories)\n",
    "  train_loss = tf_agent.train(experience=trajectories)  \n",
    "\n",
    "  replay_buffer.clear()\n",
    "  #wait = input()\n",
    "  step = tf_agent.train_step_counter.numpy()\n",
    "  print(\">>>>>>>>>>>>>\", step)\n",
    "\n",
    "  if step % log_interval == 0:\n",
    "    print('step = {0}: loss = {1}'.format(step, train_loss.loss))\n",
    "\n",
    "  if step % eval_interval == 0:\n",
    "    avg_return = compute_avg_return(eval_env, tf_agent.policy, num_eval_episodes)\n",
    "    print('step = {0}: Average Return = {1}'.format(step, avg_return))\n",
    "    restart_flag = 0\n",
    "    while (restart_flag == 0):\n",
    "      try:\n",
    "        restart_nf()\n",
    "        restart_flag = 1\n",
    "      except:\n",
    "        restart_flag = 0\n",
    "\n",
    "    checkpoint_dir = os.path.join(\"./ckpts/\", 'checkpoint_test_re10')\n",
    "\n",
    "    train_checkpointer = common.Checkpointer(\n",
    "        ckpt_dir=checkpoint_dir,\n",
    "        max_to_keep=1,\n",
    "        agent=tf_agent,\n",
    "        policy=tf_agent.policy,\n",
    "        replay_buffer=replay_buffer,\n",
    "        global_step=global_step\n",
    "    )\n",
    "\n",
    "    tf_policy_saver = policy_saver.PolicySaver(tf_agent.policy)\n",
    "    policy_dir = os.path.join(\"./ckpts/\", 'checkpoint_test_re10')\n",
    "    tf_policy_saver.save(policy_dir)\n",
    "    train_checkpointer.save(global_step)\n",
    "    saved_policy = tf.saved_model.load(policy_dir)\n",
    "    returns.append(avg_return)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
